{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy as sp\n",
    "import seaborn as sn\n",
    "%matplotlib inline\n",
    "from sklearn import preprocessing\n",
    "from sklearn.utils import shuffle\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    #Load the row data from the file \n",
    "    data = pd.read_csv('../data/Africa_Vectors_database_1898-2016.csv', sep = ',', encoding = \"ISO-8859-1\")\n",
    "    \n",
    "    # remove white spaces at the begining and end of column names and labels in the columns\n",
    "    Region = 'GAUL_Admin2'\n",
    "    data.columns = data.columns.str.strip()\n",
    "    data['Country']= data['Country'].str.strip()\n",
    "    data[Region]= data[Region].str.strip()\n",
    "    data['Adults/Larvae']= data['Adults/Larvae'].str.strip()\n",
    "\n",
    "    # convert the 3 columns to Upper case\n",
    "    data['Country'] = data['Country'].str.upper()\n",
    "    data[Region] = data[Region].str.upper()\n",
    "    data['Adults/Larvae'] = data['Adults/Larvae'].str.upper()\n",
    "\n",
    "    #change the column name of Full_Name to Region\n",
    "    data = data.rename(columns={Region: 'Region'})\n",
    "\n",
    "    #Taking the mean over the two years, round is to make sure we do not have decimals in years \n",
    "    data['Year'] = list(round(data[['YeStart', 'YeEnd']].mean(axis=1)))\n",
    "\n",
    "    #Selecting the features to keep\n",
    "    features =['Country','Region', 'Lat', 'Long','Year', 'An gambiae_complex', 'An gambiae ss', 'SS M Form (An colluzzi or Mopti forms)', 'SS S Form (savanah or Bamako forms)','An arabiensis','An. melas','An. merus','An bwambae','An funestus  s.l','An funestus s.s. (specified)','An rivulorum','An leesoni','An parensis','An vaneedeni','An nili s.l','An moucheti s.l','An pharoensis','An hancocki','An mascarensis','An marshalli','An squamous','An wellcomei','An rufipes','An coustani s.l','An ziemanni','An paludis','Adults/Larvae']\n",
    "\n",
    "    #Returning a dataset with only the features kept\n",
    "    featured_data= data[features]\n",
    "\n",
    "    #remove records with Lat,Long missing values \n",
    "    featured_data = featured_data.dropna(axis=0, subset=['Lat'])\n",
    "\n",
    "    #encoding the species classes \n",
    "    encoded_data = featured_data.replace(np.nan,0).replace('Y',1)\n",
    "\n",
    "    # Reseting the  index\n",
    "    encoded_data=encoded_data.reset_index(drop=True)\n",
    "\n",
    "    #encoding the labels columns \n",
    "    # Label encoding for Country, Region, and  Adults/Larvae columns \n",
    "    le = preprocessing.LabelEncoder()\n",
    "    encoded_data['Country'] = le.fit_transform(encoded_data['Country'])\n",
    "    encoded_data['Adults/Larvae'] = le.fit_transform(encoded_data['Adults/Larvae'])\n",
    "    encoded_data['Region'] = le.fit_transform(encoded_data['Region'].astype(str))\n",
    "    \n",
    "    #normalize the data\n",
    "    #encoded_data=(encoded_data-encoded_data.mean())/encoded_data.std()\n",
    "\n",
    "    \n",
    "    #normalize the longitude and latitude \n",
    "#     encoded_data['Lat']=(encoded_data['Lat']-encoded_data['Lat'].mean())/encoded_data['Lat'].std()\n",
    "#     encoded_data['Long']=(encoded_data['Long']-encoded_data['Long'].mean())/encoded_data['Long'].std()\n",
    "#     encoded_data['Year']=(encoded_data['Year']-encoded_data['Year'].mean())/encoded_data['Year'].std()\n",
    "      \n",
    "    #feature scaling for year, longitude and latitude \n",
    "    encoded_data['Lat']=(encoded_data['Lat']-encoded_data['Lat'].min())/encoded_data['Lat'].max()\n",
    "    encoded_data['Long']=(encoded_data['Long']-encoded_data['Long'].min())/encoded_data['Long'].max()\n",
    "    encoded_data['Year']=(encoded_data['Year']-encoded_data['Year'].min())/encoded_data['Year'].max()\n",
    "\n",
    "    #convert the year column from float to int \n",
    "    #data = data.astype({'Year':'int'})\n",
    "    encoded_data = shuffle(encoded_data)\n",
    "\n",
    "    return encoded_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1a1adffc90>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into training and testing datasets \n",
    "inputs = get_data().values[:,4:] #species columns \n",
    "targets = get_data().values[:,0] #Lat & Long\n",
    "train_inputs = torch.tensor(inputs[0:9000]).float().to(device)\n",
    "train_targets = torch.tensor(targets[0:9000]).long().to(device)\n",
    "test_inputs = torch.tensor(inputs[9000:]).float().to(device)\n",
    "test_targets = torch.tensor(targets[9000:]).long().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "N,D = train_inputs.shape\n",
    "C = test_targets.shape\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class model(nn.Module):\n",
    "    def __init__(self,n_hidden):\n",
    "        super(model, self).__init__()\n",
    "        self.batch_momentum = 0.999\n",
    "        self.track_running_stats= False\n",
    "        self.block1 = nn.Sequential(\n",
    "        nn.Linear(D, n_hidden), # layer 1 \n",
    "        nn.BatchNorm1d(n_hidden,momentum=self.batch_momentum,track_running_stats=self.track_running_stats),\n",
    "        nn.ReLU(),\n",
    "        ) #100\n",
    "        \n",
    "        self.block2 = nn.Sequential(         \n",
    "        nn.Linear(n_hidden, n_hidden), # layer 2\n",
    "        nn.BatchNorm1d(n_hidden,momentum=self.batch_momentum,track_running_stats=self.track_running_stats),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(n_hidden, n_hidden), # layer 3\n",
    "        nn.BatchNorm1d(n_hidden,momentum=self.batch_momentum,track_running_stats=self.track_running_stats),\n",
    "        nn.ReLU(),\n",
    "        )\n",
    "        #100\n",
    "        \n",
    "        self.block3 = nn.Sequential(  \n",
    "        nn.Linear(n_hidden,n_hidden), # layer 4\n",
    "        nn.BatchNorm1d(n_hidden,momentum=self.batch_momentum,track_running_stats=self.track_running_stats),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(n_hidden, n_hidden), # layer 5\n",
    "        nn.BatchNorm1d(n_hidden,momentum=self.batch_momentum,track_running_stats=self.track_running_stats),\n",
    "        nn.ReLU(),\n",
    "        )#100\n",
    "        \n",
    "        self.block4 = nn.Sequential(  \n",
    "        nn.Linear(n_hidden,48), # layer 6\n",
    "        )#48\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.block1(x)\n",
    "        residual1 = x   #Save input as residual\n",
    "\n",
    "        \n",
    "        x = self.block2(x)\n",
    "        \n",
    "        x += residual1 #add input to output of block2\n",
    "        residual2 = x  #save output of block1 as residual\n",
    "        \n",
    "        x = self.block3(x)\n",
    "        x += residual2 #add input to output of block2\n",
    "        \n",
    "        x = self.block4(x)\n",
    "        \n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "test_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch,model):\n",
    "    model.train()\n",
    "    for data,target in zip(train_inputs.split(batch_size),train_targets.split(batch_size)):\n",
    "        output = model(data)\n",
    "        #print(100)\n",
    "        loss = F.nll_loss(output,target)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if(epoch%10 == 0):\n",
    "        train_losses.append(loss.item())\n",
    "        print(\"[EPOCH]: %i, [LOSS]: %.6f\" % (epoch, loss.item()))\n",
    "        \n",
    "\n",
    "def test(model):\n",
    "#     for _ in range(2):\n",
    "#         model(torch.FloatTensor(2,28))#https://discuss.pytorch.org/t/model-eval-gives-incorrect-loss-for-model-with-batchnorm-layers/7561/2\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data,target in zip(test_inputs.split(batch_size),test_targets.split(batch_size)):\n",
    "        output = model(data)\n",
    "        test_loss += F.nll_loss(output, target,reduction='sum').item()#/test_inputs.shape[0]  # sum up batch loss instead of averaging by multplying the batch size                                                           \n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum().item()\n",
    "    test_loss /= test_inputs.shape[0]\n",
    "    test_losses.append(test_loss)\n",
    "    accuracy = 100. * correct / test_inputs.shape[0]\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy:({:.0f}%)\\n'.format(test_loss,accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimization paramaters\n",
    "lr = 1e-3\n",
    "lambda_l2 = 1e-5\n",
    "nb_epoches = 1000\n",
    "batch_size =  10\n",
    "#criterion = torch.nn.MSELoss()\n",
    "Net = model(100).to(device)\n",
    "#optimizer = torch.optim.SGD(Net.parameters(), lr=lr, momentum=0.5) # built-in L2\n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=lambda_l2) # built-in L2\n",
    "optimizer = torch.optim.Adam(Net.parameters(), lr=lr, weight_decay=lambda_l2) # built-in L2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'checkpoint_c.pth.tar'\n",
    "epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in\n",
      "=> no checkpoint found at 'checkpoint_c.pth.tar'\n"
     ]
    }
   ],
   "source": [
    "# save and load the model \n",
    "# print(file_name)\n",
    "import os \n",
    "if file_name:\n",
    "        print('in')\n",
    "        if os.path.isfile(file_name):\n",
    "            print(\"=> loading checkpoint '{}'\".format(file_name))\n",
    "            checkpoint = torch.load(file_name)\n",
    "            epoch = checkpoint['epoch']\n",
    "            model.load_state_dict(checkpoint['state_dict'])\n",
    "            optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "            test_losses = checkpoint['test_losses']\n",
    "            train_losses = checkpoint['train_losses']\n",
    "            print(\"=> loaded checkpoint '{}' (epoch {})\"\n",
    "                  .format(file_name, checkpoint['epoch']))\n",
    "            model.eval() #https://discuss.pytorch.org/t/saving-and-loading-a-model-in-pytorch/2610/8\n",
    "\n",
    "        else:\n",
    "            print(\"=> no checkpoint found at '{}'\".format(file_name))\n",
    "\n",
    "def save_checkpoint(state, filename='checkpoint.pth.tar'):\n",
    "    torch.save(state, filename)\n",
    "    file_name = filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH]: 0, [LOSS]: 3.434538\n",
      "[EPOCH]: 10, [LOSS]: 2.861937\n",
      "\n",
      "Test set: Average loss: 4.9018, Accuracy:(5%)\n",
      "\n",
      "[EPOCH]: 20, [LOSS]: 1.889056\n",
      "[EPOCH]: 30, [LOSS]: 1.317184\n",
      "\n",
      "Test set: Average loss: 7.5903, Accuracy:(6%)\n",
      "\n",
      "[EPOCH]: 40, [LOSS]: 0.994318\n"
     ]
    }
   ],
   "source": [
    "while (1):\n",
    "    train(epoch,Net)\n",
    "    epoch += 1\n",
    "    if(epoch%20 ==0):\n",
    "        test(Net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange((len(train_losses)))*10,np.array(train_losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_checkpoint({\n",
    "            'epoch': epoch + 1,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'optimizer' : optimizer.state_dict(),\n",
    "            'train_losses': train_losses,\n",
    "            'test_losses': test_losses\n",
    "        })"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
