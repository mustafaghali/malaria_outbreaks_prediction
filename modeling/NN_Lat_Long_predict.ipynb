{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy as sp\n",
    "import seaborn as sn\n",
    "%matplotlib notebook\n",
    "from sklearn import preprocessing\n",
    "from sklearn.utils import shuffle\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    #Load the row data from the file \n",
    "    data = pd.read_csv('../data/Africa_Vectors_database_1898-2016.csv', sep = ',', encoding = \"ISO-8859-1\")\n",
    "    \n",
    "    # remove white spaces at the begining and end of column names and labels in the columns\n",
    "    Region = 'GAUL_Admin2'\n",
    "    data.columns = data.columns.str.strip()\n",
    "    data['Country']= data['Country'].str.strip()\n",
    "    data[Region]= data[Region].str.strip()\n",
    "    data['Adults/Larvae']= data['Adults/Larvae'].str.strip()\n",
    "\n",
    "    # convert the 3 columns to Upper case\n",
    "    data['Country'] = data['Country'].str.upper()\n",
    "    data[Region] = data[Region].str.upper()\n",
    "    data['Adults/Larvae'] = data['Adults/Larvae'].str.upper()\n",
    "\n",
    "    #change the column name of Full_Name to Region\n",
    "    data = data.rename(columns={Region: 'Region'})\n",
    "\n",
    "    #Taking the mean over the two years, round is to make sure we do not have decimals in years \n",
    "    data['Year'] = list(round(data[['YeStart', 'YeEnd']].mean(axis=1)))\n",
    "\n",
    "    #Selecting the features to keep\n",
    "    features =['Country','Region', 'Lat', 'Long','Year', 'An gambiae_complex', 'An gambiae ss', 'SS M Form (An colluzzi or Mopti forms)', 'SS S Form (savanah or Bamako forms)','An arabiensis','An. melas','An. merus','An bwambae','An funestus  s.l','An funestus s.s. (specified)','An rivulorum','An leesoni','An parensis','An vaneedeni','An nili s.l','An moucheti s.l','An pharoensis','An hancocki','An mascarensis','An marshalli','An squamous','An wellcomei','An rufipes','An coustani s.l','An ziemanni','An paludis','Adults/Larvae']\n",
    "\n",
    "    #Returning a dataset with only the features kept\n",
    "    featured_data= data[features]\n",
    "\n",
    "    #remove records with Lat,Long missing values \n",
    "    featured_data = featured_data.dropna(axis=0, subset=['Lat'])\n",
    "\n",
    "    #encoding the species classes \n",
    "    encoded_data = featured_data.replace(np.nan,0).replace('Y',1)\n",
    "\n",
    "    # Reseting the  index\n",
    "    encoded_data=encoded_data.reset_index(drop=True)\n",
    "\n",
    "    #encoding the labels columns \n",
    "    # Label encoding for Country, Region, and  Adults/Larvae columns \n",
    "    le = preprocessing.LabelEncoder()\n",
    "    encoded_data['Country'] = le.fit_transform(encoded_data['Country'])\n",
    "    encoded_data['Adults/Larvae'] = le.fit_transform(encoded_data['Adults/Larvae'])\n",
    "    encoded_data['Region'] = le.fit_transform(encoded_data['Region'].astype(str))\n",
    "    \n",
    "    #normalize the data\n",
    "    #encoded_data=(encoded_data-encoded_data.mean())/encoded_data.std()\n",
    "\n",
    "    \n",
    "    #normalize the longitude and latitude \n",
    "#     encoded_data['Lat']=(encoded_data['Lat']-encoded_data['Lat'].mean())/encoded_data['Lat'].std()\n",
    "#     encoded_data['Long']=(encoded_data['Long']-encoded_data['Long'].mean())/encoded_data['Long'].std()\n",
    "#     encoded_data['Year']=(encoded_data['Year']-encoded_data['Year'].mean())/encoded_data['Year'].std()\n",
    "      \n",
    "    #feature scaling for year, longitude and latitude \n",
    "    encoded_data['Lat']=(encoded_data['Lat']-encoded_data['Lat'].min())/encoded_data['Lat'].max()\n",
    "    encoded_data['Long']=(encoded_data['Long']-encoded_data['Long'].min())/encoded_data['Long'].max()\n",
    "    encoded_data['Year']=(encoded_data['Year']-encoded_data['Year'].min())/encoded_data['Year'].max()\n",
    "\n",
    "    #convert the year column from float to int \n",
    "    #data = data.astype({'Year':'int'})\n",
    "    encoded_data = shuffle(encoded_data)\n",
    "\n",
    "    return encoded_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1a1ef53dd0>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 12325\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into training and testing datasets \n",
    "inputs = get_data().values[:,4:] #species columns + year\n",
    "targets = get_data().values[:,2:4] #Lat & Long\n",
    "train_inputs = torch.tensor(inputs[0:9000]).float().to(device)\n",
    "train_targets = torch.tensor(targets[0:9000]).float().to(device)\n",
    "test_inputs = torch.tensor(inputs[9000:]).float().to(device)\n",
    "test_targets = torch.tensor(targets[9000:]).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "N,D = train_inputs.shape\n",
    "C = test_targets.shape[1]\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class model(nn.Module):\n",
    "    def __init__(self,n_hidden):\n",
    "        super(model, self).__init__()\n",
    "        self.batch_momentum = 0.999\n",
    "        self.track_running_stats= False\n",
    "        self.block1 = nn.Sequential(\n",
    "        nn.Linear(D, n_hidden), # layer 1 \n",
    "        nn.BatchNorm1d(n_hidden,momentum=self.batch_momentum,track_running_stats=self.track_running_stats),\n",
    "        nn.ReLU(),\n",
    "        ) #100\n",
    "        \n",
    "        self.block2 = nn.Sequential(         \n",
    "        nn.Linear(n_hidden, n_hidden), # layer 2\n",
    "        nn.BatchNorm1d(n_hidden,momentum=self.batch_momentum,track_running_stats=self.track_running_stats),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(n_hidden, n_hidden), # layer 3\n",
    "        nn.BatchNorm1d(n_hidden,momentum=self.batch_momentum,track_running_stats=self.track_running_stats),\n",
    "        nn.ReLU(),\n",
    "        )\n",
    "        #100\n",
    "        \n",
    "        self.block3 = nn.Sequential(  \n",
    "        nn.Linear(n_hidden,n_hidden), # layer 4\n",
    "        nn.BatchNorm1d(n_hidden,momentum=self.batch_momentum,track_running_stats=self.track_running_stats),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(n_hidden, n_hidden), # layer 5\n",
    "        nn.BatchNorm1d(n_hidden,momentum=self.batch_momentum,track_running_stats=self.track_running_stats),\n",
    "        nn.ReLU(),\n",
    "        )#100\n",
    "        \n",
    "#         self.block4 = nn.Sequential(  \n",
    "#         nn.Linear(n_hidden,n_hidden), # layer 6\n",
    "#         nn.BatchNorm1d(n_hidden),  \n",
    "#         nn.ReLU(),\n",
    "#         nn.Linear(n_hidden, n_hidden), # layer 7\n",
    "#         nn.BatchNorm1d(n_hidden),\n",
    "#         nn.ReLU(),\n",
    "#         )#100\n",
    "        \n",
    "        self.block4 = nn.Sequential(  \n",
    "        nn.Linear(n_hidden,50), # layer 6\n",
    "        nn.BatchNorm1d(50,momentum=self.batch_momentum,track_running_stats=self.track_running_stats),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(50,2), # layer 7\n",
    "        )#2\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.block1(x)\n",
    "        residual1 = x   #Save input as residual\n",
    "\n",
    "        \n",
    "        x = self.block2(x)\n",
    "        \n",
    "        #x += residual1 #add input to output of block2\n",
    "        residual2 = x  #save output of block1 as residual\n",
    "        \n",
    "        x = self.block3(x)\n",
    "        #x += residual2 #add input to output of block2\n",
    "        \n",
    "        x = self.block4(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_init(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "test_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch,model):\n",
    "    model.train()\n",
    "    for data,target in zip(train_inputs.split(batch_size),train_targets.split(batch_size)):\n",
    "        output = model(data)\n",
    "        #print(100)\n",
    "        loss = criterion(output,target)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if(epoch%10 == 0):\n",
    "        train_losses.append(loss.item())\n",
    "        print(\"[EPOCH]: %i, [LOSS or MSE]: %.6f\" % (epoch, loss.item()))\n",
    "        \n",
    "\n",
    "def test(model):\n",
    "#     for _ in range(2):\n",
    "#         model(torch.FloatTensor(2,28))#https://discuss.pytorch.org/t/model-eval-gives-incorrect-loss-for-model-with-batchnorm-layers/7561/2\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    for data,target in zip(test_inputs.split(batch_size),test_targets.split(batch_size)):\n",
    "        output = model(data)\n",
    "        test_loss += (criterion(output, target) * batch_size).item()  # sum up batch loss instead of averaging by multplying the batch size                                                           \n",
    "    test_loss /= test_inputs.shape[0]\n",
    "    test_losses.append(test_loss)\n",
    "    print('\\nTest set: Average loss: {:.4f}\\n'.format(test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimization paramaters\n",
    "lr = 1e-3\n",
    "lambda_l2 = 1e-5\n",
    "nb_epoches = 1000\n",
    "batch_size =  10\n",
    "criterion = torch.nn.MSELoss()\n",
    "Net = model(100).to(device)\n",
    "optimizer = torch.optim.SGD(Net.parameters(), lr=lr, momentum=0.5) # built-in L2\n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=lambda_l2) # built-in L2\n",
    "optimizer = torch.optim.Adam(Net.parameters(), lr=lr) # built-in L2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'checkpoint.pth.tar'\n",
    "epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in\n",
      "=> no checkpoint found at 'checkpoint.pth.tar'\n"
     ]
    }
   ],
   "source": [
    "# save and load the model \n",
    "# print(file_name)\n",
    "import os \n",
    "if file_name:\n",
    "        print('in')\n",
    "        if os.path.isfile(file_name):\n",
    "            print(\"=> loading checkpoint '{}'\".format(file_name))\n",
    "            checkpoint = torch.load(file_name)\n",
    "            epoch = checkpoint['epoch']\n",
    "            model.load_state_dict(checkpoint['state_dict'])\n",
    "            optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "            test_losses = checkpoint['test_losses']\n",
    "            train_losses = checkpoint['train_losses']\n",
    "            print(\"=> loaded checkpoint '{}' (epoch {})\"\n",
    "                  .format(file_name, checkpoint['epoch']))\n",
    "            model.eval() #https://discuss.pytorch.org/t/saving-and-loading-a-model-in-pytorch/2610/8\n",
    "\n",
    "        else:\n",
    "            print(\"=> no checkpoint found at '{}'\".format(file_name))\n",
    "\n",
    "def save_checkpoint(state, filename='checkpoint.pth.tar'):\n",
    "    torch.save(state, filename)\n",
    "    file_name = filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    " #https://discuss.pytorch.org/t/model-eval-gives-incorrect-loss-for-model-with-batchnorm-layers/7561/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH]: 0, [LOSS or MSE]: 0.225131\n",
      "[EPOCH]: 10, [LOSS or MSE]: 0.183193\n",
      "\n",
      "Test set: Average loss: 0.2527\n",
      "\n",
      "[EPOCH]: 20, [LOSS or MSE]: 0.207662\n",
      "[EPOCH]: 30, [LOSS or MSE]: 0.082194\n",
      "\n",
      "Test set: Average loss: 0.3240\n",
      "\n",
      "[EPOCH]: 40, [LOSS or MSE]: 0.080016\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-71-162fd8d10696>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mNet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mepoch\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m20\u001b[0m \u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-66-2d03832d863d>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch, model)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "while (1):\n",
    "    train(epoch,Net)\n",
    "    epoch += 1\n",
    "    if(epoch%20 ==0):       \n",
    "        test(Net)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange((len(train_losses)))*10,np.array(train_losses))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_checkpoint({\n",
    "            'epoch': epoch + 1,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'optimizer' : optimizer.state_dict(),\n",
    "            'train_losses': train_losses,\n",
    "            'test_losses': test_losses\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
